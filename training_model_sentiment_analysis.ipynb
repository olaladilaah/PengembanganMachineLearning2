{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Projek Belajar Pengembangan Machine Learning - Adilah Widiasti - B244035E"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mengimpor pustaka yang diperlukan untuk pemrosesan data dan model pembelajaran mendalam\n",
        "import pandas as pd  # Mengimpor pustaka pandas untuk manipulasi dan analisis data, terutama untuk struktur data seperti DataFrame\n",
        "import numpy as np  # Mengimpor pustaka numpy untuk operasi numerik dan manipulasi array\n",
        "import tensorflow as tf  # Mengimpor pustaka TensorFlow untuk membangun dan melatih model pembelajaran mendalam\n",
        "from sklearn.model_selection import train_test_split  # Mengimpor fungsi untuk membagi dataset menjadi data pelatihan dan pengujian\n",
        "from sklearn.preprocessing import LabelEncoder  # Mengimpor LabelEncoder untuk mengubah label kategori menjadi format numerik\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # Mengimpor TfidfVectorizer untuk mengubah teks menjadi representasi numerik berbasis frekuensi\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer  # Mengimpor Tokenizer untuk mengonversi teks menjadi urutan token\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences  # Mengimpor pad_sequences untuk memastikan semua urutan memiliki panjang yang sama\n",
        "from tensorflow.keras.models import Sequential  # Mengimpor model Sequential untuk membangun model pembelajaran mendalam secara berurutan\n",
        "from tensorflow.keras.layers import Embedding, LSTM, GRU, Bidirectional, Conv1D, GlobalMaxPooling1D, Dense, Dropout, SpatialDropout1D  # Mengimpor berbagai lapisan untuk membangun arsitektur model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau  # Mengimpor callback untuk menghentikan pelatihan lebih awal dan mengurangi laju pembelajaran saat tidak ada peningkatan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZknKuwVxCEX6"
      },
      "outputs": [],
      "source": [
        "# Membaca Data Excel\n",
        "dataframe = pd.read_csv('gojek_scrapped_data.csv')  # Membaca file CSV yang berisi data yang telah diambil dari Gojek dan menyimpannya dalam DataFrame\n",
        "\n",
        "# Memisahkan Data\n",
        "features = dataframe['content_clean']  # Mengambil kolom 'content_clean' sebagai fitur (data input)\n",
        "labels = dataframe['polarity']  # Mengambil kolom 'polarity' sebagai label (data target)\n",
        "\n",
        "# Mengubah label menjadi format numerik\n",
        "encoder = LabelEncoder()  # Membuat objek LabelEncoder untuk mengubah label kategori menjadi format numerik\n",
        "encoded_labels = encoder.fit_transform(labels)  # Mengubah label menjadi format numerik dan menyimpannya dalam variabel encoded_labels\n",
        "\n",
        "# Pembagian Data: Skema 1 (80-20)\n",
        "features_train_1, features_test_1, labels_train_1, labels_test_1 = train_test_split(features, encoded_labels, test_size=0.2, random_state=42)  \n",
        "# Membagi data menjadi data pelatihan (80%) dan data pengujian (20%) untuk skema 1\n",
        "\n",
        "# Pembagian Data: Skema 2 (70-30)\n",
        "features_train_2, features_test_2, labels_train_2, labels_test_2 = train_test_split(features, encoded_labels, test_size=0.3, random_state=42)  \n",
        "# Membagi data menjadi data pelatihan (70%) dan data pengujian (30%) untuk skema 2\n",
        "\n",
        "# Tokenisasi dan Padding untuk Skema 1\n",
        "tokenizer_1 = Tokenizer()  # Membuat objek Tokenizer untuk mengonversi teks menjadi urutan token\n",
        "tokenizer_1.fit_on_texts(features_train_1)  # Melatih tokenizer pada data pelatihan\n",
        "sequences_train_1 = tokenizer_1.texts_to_sequences(features_train_1)  # Mengonversi teks pelatihan menjadi urutan angka\n",
        "sequences_test_1 = tokenizer_1.texts_to_sequences(features_test_1)  # Mengonversi teks pengujian menjadi urutan angka\n",
        "\n",
        "max_length = 100  # Menentukan panjang maksimum urutan\n",
        "size_vocab_1 = len(tokenizer_1.word_index) + 1  # Menghitung ukuran kosakata (jumlah kata unik + 1 untuk padding)\n",
        "padded_train_1 = pad_sequences(sequences_train_1, maxlen=max_length, padding='post')  # Melakukan padding pada urutan pelatihan\n",
        "padded_test_1 = pad_sequences(sequences_test_1, maxlen=max_length, padding='post')  # Melakukan padding pada urutan pengujian\n",
        "\n",
        "# Tokenisasi dan Padding untuk Skema 2\n",
        "tokenizer_2 = Tokenizer()  # Membuat objek Tokenizer untuk skema 2\n",
        "tokenizer_2.fit_on_texts(features_train_2)  # Melatih tokenizer pada data pelatihan skema 2\n",
        "sequences_train_2 = tokenizer_2.texts_to_sequences(features_train_2)  # Mengonversi teks pelatihan skema 2 menjadi urutan angka\n",
        "sequences_test_2 = tokenizer_2.texts_to_sequences(features_test_2)  # Mengonversi teks pengujian skema 2 menjadi urutan angka\n",
        "\n",
        "size_vocab_2 = len(tokenizer_2.word_index) + 1  # Menghitung ukuran kosakata untuk skema 2\n",
        "padded_train_2 = pad_sequences(sequences_train_2, maxlen=max_length, padding='post')  # Melakukan padding pada urutan pelatihan skema 2\n",
        "padded_test_2 = pad_sequences(sequences_test_2, maxlen=max_length, padding='post')  # Melakukan padding pada urutan pengujian skema 2\n",
        "\n",
        "# Ekstraksi Fitur menggunakan TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Membuat objek TfidfVectorizer untuk mengubah teks menjadi representasi numerik berbasis frekuensi\n",
        "tfidf_features = tfidf_vectorizer.fit_transform(features).toarray()  # Menghitung fitur TF-IDF dari data fitur dan mengonversinya menjadi array\n",
        "\n",
        "# Memisahkan Data TF-IDF\n",
        "tfidf_train, tfidf_test, labels_train_tfidf, labels_test_tfidf = train_test_split(tfidf_features, encoded_labels, test_size=0.2, random_state=42)  \n",
        "# Membagi data TF-IDF menjadi data pelatihan (80%) dan data pengujian (20%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0VWJapHCVDy",
        "outputId": "79822951-9ac3-498d-9db4-0f7f42f5c475"
      },
      "outputs": [],
      "source": [
        "# Pelatihan Model CNN\n",
        "# Melatih dan mengevaluasi CNN dengan menggunakan TF-IDF\n",
        "model_cnn_tfidf = Sequential()  # Membuat model Sequential untuk arsitektur CNN\n",
        "model_cnn_tfidf.add(Dense(128, activation='relu', input_shape=(X_train_tfidf.shape[1],)))  \n",
        "# Menambahkan lapisan Dense dengan 128 neuron dan fungsi aktivasi ReLU, serta menentukan bentuk input berdasarkan jumlah fitur TF-IDF\n",
        "\n",
        "model_cnn_tfidf.add(Dropout(0.5))  # Menambahkan lapisan Dropout untuk mengurangi overfitting dengan mengabaikan 50% neuron secara acak\n",
        "model_cnn_tfidf.add(Dense(64, activation='relu'))  # Menambahkan lapisan Dense kedua dengan 64 neuron dan fungsi aktivasi ReLU\n",
        "model_cnn_tfidf.add(Dropout(0.5))  # Menambahkan lapisan Dropout kedua untuk mengurangi overfitting\n",
        "\n",
        "model_cnn_tfidf.add(Dense(3, activation='softmax'))  # Menambahkan lapisan output dengan 3 neuron (untuk 3 kelas) dan fungsi aktivasi softmax untuk klasifikasi multi-kelas\n",
        "\n",
        "# Mengompilasi model\n",
        "model_cnn_tfidf.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  \n",
        "# Mengompilasi model dengan menggunakan fungsi loss sparse_categorical_crossentropy, optimizer Adam, dan metrik akurasi\n",
        "\n",
        "# Mengatur Early Stopping dan Reduce Learning Rate\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)  \n",
        "# Mengatur EarlyStopping untuk menghentikan pelatihan jika tidak ada perbaikan pada loss validasi selama 2 epoch dan mengembalikan bobot terbaik\n",
        "lr_reduction = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)  \n",
        "# Mengatur pengurangan laju pembelajaran jika tidak ada perbaikan pada loss validasi selama 3 epoch, mengurangi laju pembelajaran dengan faktor 0.2\n",
        "\n",
        "# Melatih model\n",
        "training_history_cnn_tfidf = model_cnn_tfidf.fit(X_train_tfidf, y_train_tfidf, epochs=30, batch_size=32, validation_split=0.2, callbacks=[early_stop, lr_reduction])  \n",
        "# Melatih model dengan data pelatihan TF-IDF, menggunakan 30 epoch, ukuran batch 32, dan 20% data untuk validasi, serta menerapkan callback early stopping dan pengurangan laju pembelajaran\n",
        "\n",
        "# Mengevaluasi model\n",
        "loss_cnn_tfidf, accuracy_cnn_tfidf = model_cnn_tfidf.evaluate(X_test_tfidf, y_test_tfidf)  \n",
        "# Mengevaluasi model menggunakan data pengujian TF-IDF dan menyimpan nilai loss dan akurasi\n",
        "\n",
        "# Menghitung akurasi pelatihan\n",
        "train_accuracy_cnn_tfidf = max(training_history_cnn_tfidf.history['accuracy']) * 100  \n",
        "# Menghitung akurasi maksimum selama pelatihan dari riwayat pelatihan dan mengalikannya dengan 100 untuk mendapatkan persentase\n",
        "\n",
        "# Menampilkan hasil\n",
        "print(f'Akurasi Pelatihan Model CNN (80-20 Split, TF-IDF): {train_accuracy_cnn_tfidf:.2f}%')  \n",
        "# Menampilkan akurasi pelatihan model CNN\n",
        "print(f'Akurasi Pengujian Model CNN (80-20 Split, TF-IDF): {accuracy_cnn_tfidf * 100:.2f}%')  \n",
        "# Menampilkan akurasi pengujian model CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcBHVY1Psl4U",
        "outputId": "1ce34636-3628-48ae-8596-7e9dc04ce825"
      },
      "outputs": [],
      "source": [
        "# Inferensi dengan CNN (menggunakan TF-IDF)\n",
        "def infer_with_cnn_tfidf(model, vectorizer, input_text):\n",
        "    # Mengubah teks input menjadi representasi TF-IDF\n",
        "    tfidf_input = vectorizer.transform([input_text]).toarray()  \n",
        "    # Menggunakan vectorizer untuk mengubah teks input menjadi representasi TF-IDF dan mengonversinya menjadi array\n",
        "\n",
        "    # Melakukan prediksi menggunakan model CNN\n",
        "    prediction = model.predict(tfidf_input)  \n",
        "    # Menggunakan model CNN untuk memprediksi kelas dari representasi TF-IDF input\n",
        "\n",
        "    # Mengembalikan kelas dengan probabilitas tertinggi\n",
        "    return np.argmax(prediction, axis=1)[0]  \n",
        "    # Mengembalikan indeks kelas dengan probabilitas tertinggi dari hasil prediksi\n",
        "\n",
        "# Contoh teks untuk inferensi\n",
        "sample_text = \"Aplikasinya jelek, masa saya mau order ojek gak bisa.\"  \n",
        "# Mendefinisikan contoh teks yang akan digunakan untuk inferensi\n",
        "\n",
        "# Melakukan prediksi dengan CNN (TF-IDF)\n",
        "predicted_class_cnn_tfidf = infer_with_cnn_tfidf(model_cnn_tfidf, tfidf_vectorizer, sample_text)  \n",
        "# Memanggil fungsi infer_with_cnn_tfidf untuk mendapatkan prediksi kelas dari teks contoh\n",
        "\n",
        "# Menampilkan hasil prediksi\n",
        "print(f'Prediksi Model CNN (80-20 Split, TF-IDF): {label_encoder.inverse_transform([predicted_class_cnn_tfidf])[0]}')  \n",
        "# Menggunakan label_encoder untuk mengonversi indeks kelas yang diprediksi kembali ke label asli dan menampilkannya"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVIlSV5HCpwc",
        "outputId": "e3cec44e-396e-4115-ea83-b7e62040b177"
      },
      "outputs": [],
      "source": [
        "# Pelatihan Model Bi-GRU\n",
        "def create_bigru_model(vocab_size, max_length):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length))\n",
        "    model.add(Bidirectional(GRU(128, dropout=0.4, recurrent_dropout=0.4)))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def train_and_evaluate_bigru(X_train, y_train, X_test, y_test, vocab_size, max_length):\n",
        "    model = create_bigru_model(vocab_size, max_length)\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "    lr_reduction = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)\n",
        "    \n",
        "    # Melatih model\n",
        "    training_history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2, callbacks=[early_stopping, lr_reduction])\n",
        "    \n",
        "    # Mengevaluasi model\n",
        "    loss, accuracy = model.evaluate(X_test, y_test)\n",
        "    max_train_accuracy = max(training_history.history['accuracy']) * 100\n",
        "    return model, max_train_accuracy, accuracy * 100\n",
        "\n",
        "# Melatih dan mengevaluasi Bi-GRU untuk Skema 1\n",
        "bigru_model_scheme_1, train_accuracy_bigru_scheme_1, test_accuracy_bigru_scheme_1 = train_and_evaluate_bigru(X_train_pad_1, y_train_1, X_test_pad_1, y_test_1, vocab_size_1, max_len)\n",
        "print(f'Akurasi Pelatihan Model Bi-GRU (80-20 Split): {train_accuracy_bigru_scheme_1:.2f}%')\n",
        "print(f'Akurasi Pengujian Model Bi-GRU (80-20 Split): {test_accuracy_bigru_scheme_1:.2f}%')\n",
        "\n",
        "# Melatih dan mengevaluasi Bi-GRU untuk Skema 2\n",
        "bigru_model_scheme_2, train_accuracy_bigru_scheme_2, test_accuracy_bigru_scheme_2 = train_and_evaluate_bigru(X_train_pad_2, y_train_2, X_test_pad_2, y_test_2, vocab_size_2, max_len)\n",
        "print(f'Akurasi Pelatihan Model Bi-GRU (70-30 Split): {train_accuracy_bigru_scheme_2:.2f}%')\n",
        "print(f'Akurasi Pengujian Model Bi-GRU (70-30 Split): {test_accuracy_bigru_scheme_2:.2f}%')\n",
        "\n",
        "# Melatih dan mengevaluasi Bi-GRU dengan TF-IDF\n",
        "bigru_model_tfidf = Sequential()\n",
        "bigru_model_tfidf.add(Dense(128, activation='relu', input_shape=(X_train_tfidf.shape[1],)))\n",
        "bigru_model_tfidf.add(Dropout(0.5))\n",
        "bigru_model_tfidf.add(Dense(64, activation='relu'))\n",
        "bigru_model_tfidf.add(Dropout(0.5))\n",
        "bigru_model_tfidf.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Mengompilasi model\n",
        "bigru_model_tfidf.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "early_stopping_tfidf = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "lr_reduction_tfidf = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)\n",
        "\n",
        "# Melatih model TF-IDF\n",
        "history_bigru_tfidf = bigru_model_tfidf.fit(X_train_tfidf, y_train_tfidf, epochs=30, batch_size=32, validation_split=0.2, callbacks=[early_stopping_tfidf, lr_reduction_tfidf])\n",
        "\n",
        "# Mengevaluasi model TF-IDF\n",
        "loss_bigru_tfidf, accuracy_bigru_tfidf = bigru_model_tfidf.evaluate(X_test_tfidf, y_test_tfidf)\n",
        "train_accuracy_bigru_tfidf = max(history_bigru_tfidf.history['accuracy']) * 100\n",
        "\n",
        "# Menampilkan hasil\n",
        "print(f'Akurasi Pelatihan Model Bi-GRU (80-20 Split, TF-IDF): {train_accuracy_bigru_tfidf:.2f}%')\n",
        "print(f'Akurasi Pengujian Model Bi-GRU (80-20 Split, TF-IDF): {accuracy_bigru_tfidf * 100:.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1A5GVZ665Vf",
        "outputId": "22bc23a2-859c-407b-9124-c30c3a0bd150"
      },
      "outputs": [],
      "source": [
        "# Inferensi dengan Bi-GRU (Skema 80-20)\n",
        "def predict_with_bigru_scheme_1(model, tokenizer, input_text, max_length):\n",
        "    # Mengubah teks input menjadi urutan angka\n",
        "    sequence = tokenizer.texts_to_sequences([input_text])\n",
        "    # Melakukan padding pada urutan\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
        "    # Melakukan prediksi menggunakan model\n",
        "    prediction = model.predict(padded_sequence)\n",
        "    # Mengembalikan kelas dengan probabilitas tertinggi\n",
        "    return np.argmax(prediction, axis=1)[0]\n",
        "\n",
        "# Inferensi dengan Bi-GRU (Skema 70-30)\n",
        "def predict_with_bigru_scheme_2(model, tokenizer, input_text, max_length):\n",
        "    sequence = tokenizer.texts_to_sequences([input_text])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
        "    prediction = model.predict(padded_sequence)\n",
        "    return np.argmax(prediction, axis=1)[0]\n",
        "\n",
        "# Inferensi dengan Bi-GRU (menggunakan TF-IDF)\n",
        "def predict_with_bigru_tfidf(model, vectorizer, input_text):\n",
        "    # Mengubah teks input menjadi representasi TF-IDF\n",
        "    tfidf_input = vectorizer.transform([input_text]).toarray()\n",
        "    prediction = model.predict(tfidf_input)\n",
        "    return np.argmax(prediction, axis=1)[0]\n",
        "\n",
        "# Contoh teks untuk inferensi\n",
        "sample_text = \"Gojek memang terbaik, memudahkan saya mau kemana aja\"\n",
        "\n",
        "# Melakukan prediksi dengan Bi-GRU (Skema 80-20)\n",
        "prediction_bigru_scheme_1 = predict_with_bigru_scheme_1(bigru_model_1, tokenizer_1, sample_text, max_len)\n",
        "print(f'Prediksi Model Bi-GRU (Skema 80-20): {label_encoder.inverse_transform([prediction_bigru_scheme_1])[0]}')\n",
        "\n",
        "# Melakukan prediksi dengan Bi-GRU (Skema 70-30)\n",
        "prediction_bigru_scheme_2 = predict_with_bigru_scheme_2(bigru_model_2, tokenizer_2, sample_text, max_len)\n",
        "print(f'Prediksi Model Bi-GRU (Skema 70-30): {label_encoder.inverse_transform([prediction_bigru_scheme_2])[0]}')\n",
        "\n",
        "# Melakukan prediksi dengan Bi-GRU (TF-IDF)\n",
        "prediction_bigru_tfidf = predict_with_bigru_tfidf(bigru_model_tfidf, tfidf_vectorizer, sample_text)\n",
        "print(f'Prediksi Model Bi-GRU (Skema 80-20, TF-IDF): {label_encoder.inverse_transform([prediction_bigru_tfidf])[0]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rc3vpesbbB9V",
        "outputId": "695cbf87-833a-4d49-d371-823004200c04"
      },
      "outputs": [],
      "source": [
        "# Pelatihan Model LSTM\n",
        "def create_lstm_model(vocab_size, max_length):\n",
        "    # Membuat model LSTM dengan arsitektur yang ditentukan\n",
        "    model = Sequential()\n",
        "    # Menambahkan layer embedding untuk mengubah kata menjadi vektor\n",
        "    model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length))\n",
        "    # Menambahkan dropout spatial untuk mengurangi overfitting\n",
        "    model.add(SpatialDropout1D(0.4))\n",
        "    # Menambahkan layer LSTM dengan dropout untuk mengurangi overfitting\n",
        "    model.add(LSTM(128, dropout=0.4, recurrent_dropout=0.4))\n",
        "    # Menambahkan layer dense dengan fungsi aktivasi softmax untuk klasifikasi\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    # Mengompilasi model dengan loss function dan optimizer yang ditentukan\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def train_and_evaluate_lstm(X_train, y_train, X_test, y_test, vocab_size, max_length):\n",
        "    # Membuat model LSTM\n",
        "    model = create_lstm_model(vocab_size, max_length)\n",
        "    # Mengatur callback untuk early stopping dan pengurangan learning rate\n",
        "    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n",
        "    lr_reduction_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)\n",
        "    \n",
        "    # Melatih model dengan data pelatihan dan validasi\n",
        "    training_history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2, callbacks=[early_stopping_callback, lr_reduction_callback])\n",
        "    \n",
        "    # Mengevaluasi model dengan data pengujian\n",
        "    loss, accuracy = model.evaluate(X_test, y_test)\n",
        "    max_train_accuracy = max(training_history.history['accuracy']) * 100\n",
        "    return model, max_train_accuracy, accuracy * 100\n",
        "\n",
        "# Melatih dan mengevaluasi LSTM dengan TF-IDF\n",
        "lstm_model_tfidf = Sequential()\n",
        "# Menambahkan layer dense untuk model TF-IDF\n",
        "lstm_model_tfidf.add(Dense(128, activation='relu', input_shape=(X_train_tfidf.shape[1],)))\n",
        "# Menambahkan dropout untuk mengurangi overfitting\n",
        "lstm_model_tfidf.add(Dropout(0.5))\n",
        "# Menambahkan layer dense tambahan\n",
        "lstm_model_tfidf.add(Dense(64, activation='relu'))\n",
        "# Menambahkan dropout lagi\n",
        "lstm_model_tfidf.add(Dropout(0.5))\n",
        "# Menambahkan layer output dengan softmax untuk klasifikasi\n",
        "lstm_model_tfidf.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Mengompilasi model TF-IDF\n",
        "lstm_model_tfidf.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Mengatur Early Stopping dan Reduce Learning Rate untuk model TF-IDF\n",
        "early_stopping_tfidf = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "lr_reduction_tfidf = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)\n",
        "\n",
        "# Melatih model TF-IDF\n",
        "history_lstm_tfidf = lstm_model_tfidf.fit(X_train_tfidf, y_train_tfidf, epochs=30, batch_size=32, validation_split=0.2, callbacks=[early_stopping_tfidf, lr_reduction_tfidf])\n",
        "\n",
        "# Mengevaluasi model TF-IDF\n",
        "loss_lstm_tfidf, accuracy_lstm_tfidf = lstm_model_tfidf.evaluate(X_test_tfidf, y_test_tfidf)\n",
        "train_accuracy_lstm_tfidf = max(history_lstm_tfidf.history['accuracy']) * 100\n",
        "\n",
        "# Menampilkan hasil akurasi pelatihan dan pengujian\n",
        "print(f'Akurasi Pelatihan Model LSTM (80-20 Split, TF-IDF): {train_accuracy_lstm_tfidf:.2f}%')\n",
        "print(f'Akurasi Pengujian Model LSTM (80-20 Split, TF-IDF): {accuracy_lstm_tfidf * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeoyoUK8iMKS",
        "outputId": "2d747473-796c-42cc-98a8-df80083fa36c"
      },
      "outputs": [],
      "source": [
        "# Inferensi dengan LSTM (menggunakan TF-IDF)\n",
        "def predict_with_lstm_tfidf(model, vectorizer, input_text):\n",
        "    # Mengubah teks input menjadi representasi TF-IDF\n",
        "    tfidf_representation = vectorizer.transform([input_text]).toarray()\n",
        "    # Melakukan prediksi menggunakan model LSTM\n",
        "    prediction = model.predict(tfidf_representation)\n",
        "    # Mengembalikan kelas dengan probabilitas tertinggi\n",
        "    return np.argmax(prediction, axis=1)[0]\n",
        "\n",
        "# Contoh teks untuk inferensi\n",
        "sample_input = \"Aplikasinya biasa sih.\"\n",
        "\n",
        "# Melakukan prediksi dengan LSTM (TF-IDF)\n",
        "predicted_class_lstm_tfidf = predict_with_lstm_tfidf(lstm_model_tfidf, tfidf_vectorizer, sample_input)\n",
        "print(f'Prediksi Model LSTM (80-20 Split, TF-IDF): {label_encoder.inverse_transform([predicted_class_lstm_tfidf])[0]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwfYneY2SSua",
        "outputId": "3e59be35-56cc-4d80-bc2a-a6188f092e34"
      },
      "outputs": [],
      "source": [
        "# Menyimpan daftar pustaka yang terpasang ke dalam file requirements.txt\n",
        "!pip freeze > requirements.txt  # Menggunakan pip freeze untuk mencatat semua pustaka yang terinstal beserta versinya"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
